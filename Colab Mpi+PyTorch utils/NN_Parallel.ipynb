{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf *\n",
        "!pip3 install mpi4py\n",
        "!pip3 install ipyparallel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOou1URmrq9G",
        "outputId": "d80cd76d-26bd-4498-be85-9a3f75f3a0d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp38-cp38-linux_x86_64.whl size=5997370 sha256=ab519e6f3656bdb636ce740cf1166a0bcbb817f0a409818e5f8d1e9806c143d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/35/48/0b9a7076995eea5ea64a7e4bc3f0f342f453080795276264e7\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipyparallel\n",
            "  Downloading ipyparallel-8.4.1-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (6.1.12)\n",
            "Requirement already satisfied: pyzmq>=18 in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (23.2.1)\n",
            "Requirement already satisfied: ipykernel>=4.4 in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (5.3.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (4.4.2)\n",
            "Requirement already satisfied: ipython>=4 in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (7.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (4.64.1)\n",
            "Requirement already satisfied: traitlets>=4.3 in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (5.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (5.4.8)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (6.0.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from ipyparallel) (0.4)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4->ipyparallel) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4->ipyparallel) (57.4.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4->ipyparallel) (2.0.10)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4->ipyparallel) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4->ipyparallel) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4->ipyparallel) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->ipyparallel) (1.15.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipyparallel) (5.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=4->ipyparallel) (0.8.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipyparallel) (2.6.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4->ipyparallel) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython>=4->ipyparallel) (0.7.0)\n",
            "Installing collected packages: jedi, ipyparallel\n",
            "Successfully installed ipyparallel-8.4.1 jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "results = r'/content/results'\n",
        "\n",
        "list_names = ['SGD','Adam','Adagrad','RMSprop','Adadelta']\n",
        "if not os.path.exists(results):\n",
        "    os.makedirs(results)\n",
        "for name in list_names:\n",
        "  optim_dir = f'{results}/{name}'\n",
        "  if not os.path.exists(optim_dir):\n",
        "    os.makedirs(optim_dir)"
      ],
      "metadata": {
        "id": "FQqaEqJ21hE3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lXwUHQ-CamR7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "random_seed = 1"
      ],
      "metadata": {
        "id": "DZo4fQVGYhd-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader,test_loader = make_train_loader_test_loader()"
      ],
      "metadata": {
        "id": "iIGoOPFarfBe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examples = enumerate(test_loader)\n",
        "# batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "metadata": {
        "id": "bhhJsU4ar2gG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_data.shape"
      ],
      "metadata": {
        "id": "sA3l-pi-r3ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD, Adam, Adagrad,RMSprop,Adadelta"
      ],
      "metadata": {
        "id": "-H5k2ZJwsZ3N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def s_to_bitlist(s):\n",
        "      ords = (ord(c) for c in s)\n",
        "      shifts = (7, 6, 5, 4, 3, 2, 1, 0)\n",
        "      return [(o >> shift) & 1 for o in ords for shift in shifts]\n",
        "def bitlist_to_chars(bl):\n",
        "  bi = iter(bl)\n",
        "  bytes = zip(*(bi,) * 8)\n",
        "  shifts = (7, 6, 5, 4, 3, 2, 1, 0)\n",
        "  for byte in bytes:\n",
        "    yield chr(sum(bit << s for bit, s in zip(byte, shifts)))\n",
        "\n",
        "def bitlist_to_s(bl):\n",
        "  return ''.join(bitlist_to_chars(bl))\n",
        "\n",
        "a = s_to_bitlist('Hi')\n",
        "\n",
        "bitlist_to_s(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g_rGOcMWsk_Q",
        "outputId": "f8f2e4e7-622d-440c-c958-1c14860e199f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training parallel"
      ],
      "metadata": {
        "id": "q9eNuN1S87SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "my_str = \"hello\"\n",
        "my_str_as_bytes = np.array(bytes(my_str,'UTF-8'))\n",
        "print(type(my_str_as_bytes)) # ensure it is byte representation\n",
        "my_decoded_str = my_str_as_bytes.decode()\n",
        "print(type(my_decoded_str)) # ensure it is string representation"
      ],
      "metadata": {
        "id": "XBQ6jE5NxnQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %localhost slots=4\n",
        "! mpirun -n 4 --allow-run-as-root --oversubscribe python3  mpiTrain.py"
      ],
      "metadata": {
        "id": "yi3xPxGP3hkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ce3c64-644d-44b6-ca34-37a9deef5061"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer: Adam - Start\n",
            "Optimizer: Adagrad - Start\n",
            "Optimizer: SGD - Start\n",
            "\n",
            "Test set: Avg. loss: 2.3044, Accuracy: 986/10000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.287512\n",
            "\n",
            "Test set: Avg. loss: 2.2976, Accuracy: 906/10000 (9%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.321400\n",
            "\n",
            "Test set: Avg. loss: 2.3218, Accuracy: 622/10000 (6%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299502\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.300504\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.249135\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.273666\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.113180\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.235998\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.329293\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.955084\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.242866\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.672861\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.355936\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.190251\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.217985\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.197073\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.328986\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.304917\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.282928\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.116789\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.308051\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.108813\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.046946\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.299597\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.106122\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.051770\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.347034\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.012726\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.050724\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.305934\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.051827\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.687724\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.321483\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.962530\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.934840\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.287544\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.907874\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.591903\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.279866\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.950884\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.730887\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.241577\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.914072\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.366187\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.682739\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.817544\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.285970\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.626846\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.714594\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.444950\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.314647\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.677412\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.830902\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.357799\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.674818\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.654868\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.815003\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.302335\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.817636\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.332498\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.350013\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.711882\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.296438\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.654623\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.689408\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.330747\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.421914\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.589697\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.452946\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.305521\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.679778\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.317452\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.470329\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.646200\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.317999\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.408731\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.551778\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.305157\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.452542\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.613322\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.340266\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.503195\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.523437\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.283381\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.287374\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.392778\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.336228\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.422543\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.674738\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.304892\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.271714\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.457390\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.300885\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.521037\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.606516\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.328015\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.029238\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.639847\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.330745\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.464072\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.481404\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.317909\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.495255\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.315069\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.667943\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.271772\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.558909\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.440271\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.296912\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.497873\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.399995\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.285687\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.294116\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.328587\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.303736\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.378974\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.292040\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.263743\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.504028\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.379415\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.270215\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.272803\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.288750\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.417433\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.273187\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.495515\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.487442\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.276120\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.257449\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.511877\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.283856\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.261431\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.372538\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.266721\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.054989\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.443517\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.309972\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.450049\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.386018\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.282091\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.061310\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.325381\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.270705\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.242738\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.369682\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.253940\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.265560\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.416535\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.282347\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.291116\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.266509\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.218584\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.213323\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.296179\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.411781\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.267583\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.439754\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.294531\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.428048\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.484163\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.271809\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.167725\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.440475\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.255843\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.285732\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.478126\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.184444\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.268322\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.650276\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.255643\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.286806\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.330162\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.096344\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.252257\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.304201\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.261399\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.257607\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.179005\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.270505\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.151173\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.476403\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.221586\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.286107\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.343374\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.248014\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.952084\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.279640\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.274309\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.150406\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.412816\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.292028\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.152841\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.361739\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.244023\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.036877\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.268556\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.855196\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.247123\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.207822\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.075758\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.287346\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.411125\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.247344\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.146249\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.187718\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.225853\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.220244\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.459002\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.254709\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.297755\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.306746\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.239856\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.139656\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.232736\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.247034\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.189825\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.291075\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.242693\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.074290\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.234835\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.266141\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.041831\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.233592\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.211642\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.090924\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.238446\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.229553\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.050651\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.434550\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.245250\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.941402\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.277416\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.157596\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.082957\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.192717\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.429300\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.171653\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.386572\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.222244\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.208492\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.301541\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.252709\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.666073\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.264181\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.233696\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.375641\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.133629\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.203971\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.316667\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.897188\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 2.255426\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.296636\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.055562\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 2.218418\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.378812\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.961296\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.227120\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.222182\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.981147\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.244467\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.273150\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.977384\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 2.231242\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.462509\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.027303\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.264848\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.252097\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.943042\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 2.230529\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.284914\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.169560\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.164119\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.833728\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.248116\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 2.246671\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.768288\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.201423\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 2.182127\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.482902\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.835304\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.212842\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 2.227274\n",
            "\n",
            "Test set: Avg. loss: 0.0975, Accuracy: 9716/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.396144\n",
            "\n",
            "Test set: Avg. loss: 0.6094, Accuracy: 8653/10000 (87%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.912036\n",
            "\n",
            "Test set: Avg. loss: 2.1867, Accuracy: 3749/10000 (37%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.236869\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.176724\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.945022\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 2.207111\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.252914\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.892268\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 2.225298\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.361528\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.925814\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 2.206911\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.130818\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.925146\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 2.217713\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.146140\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.838578\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.196604\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.243293\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.845433\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 2.195100\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.292103\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.001200\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 2.213840\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.077707\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.304464\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 2.180292\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.070687\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.322846\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 2.172802\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.956194\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.304509\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.237013\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.890957\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.213846\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 2.216352\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.700227\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.227085\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 2.191719\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.306712\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.900845\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.765322\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 2.188493\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.106502\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.869125\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 2.172468\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.320490\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.928477\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.191222\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.261866\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.779706\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.190147\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 2.151935\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.733647\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.195290\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 2.185480\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.418781\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 1.128873\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 2.157949\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.862497\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.502614\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 2.152192\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.784687\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.251639\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.785815\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.216368\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.270044\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 2.173956\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.901514\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.243008\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 2.206973\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.986142\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.346963\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.123627\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.984331\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.288422\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 2.168041\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.890025\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.352330\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 2.136479\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.886410\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.298146\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 2.163158\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.858707\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.180844\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 2.046008\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.857146\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.285250\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 2.155013\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.886526\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.202653\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 2.162395\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.312749\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.878170\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.189480\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.180835\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.028060\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 2.111941\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.302152\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.865328\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 2.074759\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.249323\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.863901\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 2.174830\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.154477\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 2.099875\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.987628\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.274615\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 2.119667\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.852180\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.173686\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 2.108332\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.919242\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.226637\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 1.044025\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 2.161661\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.250285\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 1.050344\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 2.010779\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.159495\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.701224\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 2.086200\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 1.030135\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.289821\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.100015\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.134162\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.856443\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 2.097853\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.229976\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.763305\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 2.098837\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.205055\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.894496\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 2.093020\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.166773\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.800109\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 2.117463\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.154545\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.714279\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 2.063370\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.317944\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.989462\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 2.036748\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.167639\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.815382\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.414321\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 2.104308\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.883543\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.256893\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 2.059218\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.127278\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.146625\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 2.061941\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.809873\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.160258\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.085528\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.790044\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.284728\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 2.025348\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.948943\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.118151\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 2.037828\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 1.084687\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.294851\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 2.022026\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.598195\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.463928\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 2.125299\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.970744\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.414748\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.991712\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.933323\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.091696\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 2.023587\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.771793\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.326626\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 1.972897\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.792237\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.110118\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 1.972289\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.727130\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 1.942260\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.467158\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.979376\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.871329\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.200570\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.697719\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 2.048236\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.101385\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.726057\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.371505\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 2.010294\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.900434\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.813649\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 1.960984\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.092917\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 1.033411\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 2.001369\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.351382\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 2.059743\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.789094\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.175479\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 2.065117\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.622287\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.135692\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 2.031045\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.197873\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.959390\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 1.959690\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.202853\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.782331\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 1.939581\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.199800\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.793199\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.005829\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.270827\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.847996\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 1.913702\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.266095\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.823965\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 1.911166\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.135254\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.762659\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 1.921705\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.333304\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.779229\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 1.981669\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.351830\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.692190\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 2.041525\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.244920\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.026129\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.628050\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 1.959818\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.271160\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.804157\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.900584\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 2.086426\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.151983\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.139630\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.767917\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 1.885708\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.150352\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.727429\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.947705\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.762522\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.226147\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 1.877262\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.729101\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 1.918985\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.161123\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.856000\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 1.907932\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.235473\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.812439\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.359861\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 1.859460\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.770548\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.261630\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 1.900428\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.810557\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.243765\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 1.934255\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.268161\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.867838\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 1.848113\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.198660\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.910072\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 1.937400\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.007533\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.274758\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 1.964354\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.801916\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.847447\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.182117\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 1.116752\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.251642\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 1.772815\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.808125\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 1.837445\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.164523\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 1.785424\n",
            "\n",
            "Test set: Avg. loss: 0.4482, Accuracy: 8949/10000 (89%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.679383\n",
            "\n",
            "Test set: Avg. loss: 0.0688, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.149857\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.926895\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.100767\n",
            "\n",
            "Test set: Avg. loss: 1.6571, Accuracy: 6994/10000 (70%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.017177\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.665291\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.196130\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 1.806955\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 1.000787\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 1.781074\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.178642\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.679057\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 1.966295\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.052681\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.774681\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 1.824419\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.175081\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.514126\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 1.945096\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.209198\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.736187\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 1.683133\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.254150\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.620994\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 1.764100\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.271058\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.820662\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 1.852651\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.295402\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.800074\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 1.751613\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.256082\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.907974\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.767112\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.251655\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.641284\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 1.712140\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.201785\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.828394\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 1.743030\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.096459\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.864096\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 1.754454\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.222877\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.698198\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 1.763074\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.101590\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.793002\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 1.795570\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.265719\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.755834\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 1.724858\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 1.786044\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.183874\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.924543\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 1.774774\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.328292\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.859381\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.340424\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 1.787449\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.763862\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.176122\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.729510\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.829726\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.174259\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 1.807330\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.970425\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.086968\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 1.745632\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.693074\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.138976\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 1.688420\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.549652\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.128641\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 1.699582\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.588216\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.095881\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.604643\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.792575\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.245570\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 1.659033\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.801288\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.652145\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 1.746118\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.571810\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 1.499871\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.100508\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.841441\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 1.736446\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.130307\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.903521\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.790802\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.089246\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.928072\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 1.642082\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.276076\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.700776\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.910550\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.132172\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 1.613745\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.805876\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.245431\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 1.615985\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.831274\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 1.532024\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.136204\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.599467\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 1.840738\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.135856\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.852754\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 1.664949\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.297219\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.715763\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 1.533340\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.111816\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 1.550399\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.741882\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.182482\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.564572\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.299208\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.184969\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.704645\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 1.610043\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.089927\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 1.797117\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.966226\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 1.562793\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.131521\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.773302\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 1.424424\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.314932\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.826115\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 1.380659\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.245036\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.622283\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.161421\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 1.510954\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.727019\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 1.323856\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.816340\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.136552\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 1.625315\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.314780\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.685868\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 1.568370\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.285014\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.846804\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.555136\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.242736\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.860939\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.299668\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 1.507927\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.747220\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 1.546502\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.234989\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.613314\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 1.382756\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.101504\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.584077\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 1.597668\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.101562\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.576308\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.172382\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 1.514337\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.825006\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.086460\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 1.427032\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 1.342712\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.083555\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.582695\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.420160\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 1.420328\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.759668\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.257202\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 1.432037\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.710497\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.238827\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.474166\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.565010\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 1.456413\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.133821\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.619626\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 1.424417\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.116822\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.854304\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 1.299761\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.921138\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.277664\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 1.217016\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.682763\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.340001\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.678974\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 1.324830\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.083386\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.510996\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.051350\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 1.555339\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.700398\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.292325\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 1.344440\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.945548\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.521026\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 1.554621\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.679778\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.164684\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 1.415546\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.594042\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.097027\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.341335\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.740800\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.252522\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.723060\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 1.335709\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.170831\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.748413\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.288820\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 1.311803\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 1.417091\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.723596\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.344821\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 1.388864\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.720303\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.263428\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.191722\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.312160\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.714855\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.124615\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 1.392223\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.722791\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.119822\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 1.279527\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.801777\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.112786\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 1.322769\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.622052\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.384434\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 1.600022\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.760792\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.219410\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.317060\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.830911\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.339146\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 1.378541\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.863913\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.203270\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 1.333947\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.837240\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.397166\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 1.526684\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.623977\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.324622\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 1.334138\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.930659\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.367165\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 1.303673\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.842613\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.225018\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 1.487379\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.696118\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.177782\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 1.260670\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.930444\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.128142\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 1.318981\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.684262\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.174985\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 1.341233\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.200495\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.800820\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.339842\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.350727\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.593621\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 1.557804\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.670795\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.250207\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 1.480507\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 1.114287\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.172450\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 1.298526\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.660859\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.114538\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.646700\n",
            "\n",
            "Test set: Avg. loss: 0.9318, Accuracy: 7973/10000 (80%)\n",
            "\n",
            "Optimizer: SGD - End\n",
            "Time:231.3041 sec\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.0611, Accuracy: 9817/10000 (98%)\n",
            "\n",
            "Optimizer: Adam - End\n",
            "Time:231.4894 sec\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.3793, Accuracy: 9060/10000 (91%)\n",
            "\n",
            "Optimizer: Adagrad - End\n",
            "Time:232.1297 sec\n",
            "\n",
            "mpiTrain.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "mpiTrain.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Time(s): 235.08885346900001\n",
            "mpiTrain.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it. With just 3 epochs of training we already managed to achieve 97% accuracy on the test set! We started out with randomly initialized parameters and as expected only got about 10% accuracy on the test set before starting the training.\n",
        "\n",
        "Let's plot our training curve."
      ],
      "metadata": {
        "id": "mFVO9Qkx8Vjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.offline as offline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "titles_names = [f'Plot {optimizer}' for optimizer in list_names]\n",
        "# len(list_names), 1\n",
        "fig = make_subplots(\n",
        "    rows=len(list_names), cols=1,\n",
        "    subplot_titles=titles_names)\n",
        "\n",
        "for i in range(4-1):\n",
        "    name = list_names[i]\n",
        "    with open(f'/content/results/{name}/train_losses.txt', 'r') as file:\n",
        "        train_losses = list(map(float,file.read().replace('\\n', '').replace('[','').replace(']','').split(sep=', ')))\n",
        "    \n",
        "    with open(f'/content/results/{name}/train_counter.txt', 'r') as file:\n",
        "        train_counter = list(map(float,file.read().replace('\\n', '').replace('[','').replace(']','').split(sep=', ')))\n",
        "\n",
        "    with open(f'/content/results/{name}/test_losses.txt', 'r') as file:\n",
        "        test_losses = list(map(float,file.read().replace('\\n', '').replace('[','').replace(']','').split(sep=', ')))\n",
        "    \n",
        "    with open(f'/content/results/{name}/test_counter.txt', 'r') as file:\n",
        "        test_counter = list(map(float,file.read().replace('\\n', '').replace('[','').replace(']','').split(sep=', '))) \n",
        "\n",
        "    \n",
        "    fig.add_trace(go.Scatter(x=train_counter,y=train_losses),               \n",
        "                  row=i+1, col=1)\n",
        "    \n",
        "    fig.add_scatter(\n",
        "        x=test_counter,\n",
        "        y=test_losses, \n",
        "        mode=\"markers\",                \n",
        "        marker=dict(size=20,color=\"LightSeaGreen\"),                \n",
        "        #name=\"a\", \n",
        "        row=i+1, col=1)\n",
        "\n",
        "fig.update_layout(height=500, width=700,\n",
        "                  title_text=\"Optimizers\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "oyyjTQ62v1Fv",
        "outputId": "21e24e05-59d6-453c-ba07-e09f0d1a9462"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"f0974c26-2089-479e-b1bd-6a43791aa5f1\" class=\"plotly-graph-div\" style=\"height:500px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f0974c26-2089-479e-b1bd-6a43791aa5f1\")) {                    Plotly.newPlot(                        \"f0974c26-2089-479e-b1bd-6a43791aa5f1\",                        [{\"x\":[0.0,640.0,1280.0,1920.0,2560.0,3200.0,3840.0,4480.0,5120.0,5760.0,6400.0,7040.0,7680.0,8320.0,8960.0,9600.0,10240.0,10880.0,11520.0,12160.0,12800.0,13440.0,14080.0,14720.0,15360.0,16000.0,16640.0,17280.0,17920.0,18560.0,19200.0,19840.0,20480.0,21120.0,21760.0,22400.0,23040.0,23680.0,24320.0,24960.0,25600.0,26240.0,26880.0,27520.0,28160.0,28800.0,29440.0,30080.0,30720.0,31360.0,32000.0,32640.0,33280.0,33920.0,34560.0,35200.0,35840.0,36480.0,37120.0,37760.0,38400.0,39040.0,39680.0,40320.0,40960.0,41600.0,42240.0,42880.0,43520.0,44160.0,44800.0,45440.0,46080.0,46720.0,47360.0,48000.0,48640.0,49280.0,49920.0,50560.0,51200.0,51840.0,52480.0,53120.0,53760.0,54400.0,55040.0,55680.0,56320.0,56960.0,57600.0,58240.0,58880.0,59520.0,60000.0,60640.0,61280.0,61920.0,62560.0,63200.0,63840.0,64480.0,65120.0,65760.0,66400.0,67040.0,67680.0,68320.0,68960.0,69600.0,70240.0,70880.0,71520.0,72160.0,72800.0,73440.0,74080.0,74720.0,75360.0,76000.0,76640.0,77280.0,77920.0,78560.0,79200.0,79840.0,80480.0,81120.0,81760.0,82400.0,83040.0,83680.0,84320.0,84960.0,85600.0,86240.0,86880.0,87520.0,88160.0,88800.0,89440.0,90080.0,90720.0,91360.0,92000.0,92640.0,93280.0,93920.0,94560.0,95200.0,95840.0,96480.0,97120.0,97760.0,98400.0,99040.0,99680.0,100320.0,100960.0,101600.0,102240.0,102880.0,103520.0,104160.0,104800.0,105440.0,106080.0,106720.0,107360.0,108000.0,108640.0,109280.0,109920.0,110560.0,111200.0,111840.0,112480.0,113120.0,113760.0,114400.0,115040.0,115680.0,116320.0,116960.0,117600.0,118240.0,118880.0,119520.0,120000.0,120640.0,121280.0,121920.0,122560.0,123200.0,123840.0,124480.0,125120.0,125760.0,126400.0,127040.0,127680.0,128320.0,128960.0,129600.0,130240.0,130880.0,131520.0,132160.0,132800.0,133440.0,134080.0,134720.0,135360.0,136000.0,136640.0,137280.0,137920.0,138560.0,139200.0,139840.0,140480.0,141120.0,141760.0,142400.0,143040.0,143680.0,144320.0,144960.0,145600.0,146240.0,146880.0,147520.0,148160.0,148800.0,149440.0,150080.0,150720.0,151360.0,152000.0,152640.0,153280.0,153920.0,154560.0,155200.0,155840.0,156480.0,157120.0,157760.0,158400.0,159040.0,159680.0,160320.0,160960.0,161600.0,162240.0,162880.0,163520.0,164160.0,164800.0,165440.0,166080.0,166720.0,167360.0,168000.0,168640.0,169280.0,169920.0,170560.0,171200.0,171840.0,172480.0,173120.0,173760.0,174400.0,175040.0,175680.0,176320.0,176960.0,177600.0,178240.0,178880.0,179520.0],\"y\":[2.299502372741699,2.2736663818359375,2.3292930126190186,2.355936288833618,2.3289856910705566,2.304917097091675,2.308051109313965,2.2995967864990234,2.347033739089966,2.305933713912964,2.3214831352233887,2.287543535232544,2.2798662185668945,2.241576671600342,2.366187334060669,2.2859699726104736,2.3146471977233887,2.3577985763549805,2.302335262298584,2.332498073577881,2.296437978744507,2.33074688911438,2.305520534515381,2.3174517154693604,2.3179988861083984,2.305157423019409,2.340266227722168,2.2833807468414307,2.336228132247925,2.304892063140869,2.3008852005004883,2.328015089035034,2.3307454586029053,2.3179092407226562,2.2717721462249756,2.296912431716919,2.2856874465942383,2.3037357330322266,2.2637434005737305,2.2702150344848633,2.288750171661377,2.273186683654785,2.276120185852051,2.283855676651001,2.266721248626709,2.3099722862243652,2.282090663909912,2.270705461502075,2.2539403438568115,2.282346725463867,2.2665090560913086,2.2961785793304443,2.2945306301116943,2.2718091011047363,2.285731554031372,2.2683217525482178,2.2868056297302246,2.2522566318511963,2.2613985538482666,2.27050518989563,2.2215864658355713,2.248013734817505,2.2743091583251953,2.292027711868286,2.244023084640503,2.2471225261688232,2.287346363067627,2.2473442554473877,2.225853443145752,2.254708766937256,2.239856243133545,2.2470338344573975,2.242692708969116,2.234835147857666,2.2335922718048096,2.2384464740753174,2.2452497482299805,2.277416467666626,2.1927173137664795,2.2222442626953125,2.252709150314331,2.2336955070495605,2.2039713859558105,2.2554264068603516,2.2184181213378906,2.2271203994750977,2.244466543197632,2.2312419414520264,2.26484751701355,2.230529308319092,2.1641194820404053,2.246670961380005,2.1821274757385254,2.227273941040039,2.2368690967559814,2.207111358642578,2.2252979278564453,2.2069108486175537,2.217712879180908,2.196603536605835,2.1951003074645996,2.213839530944824,2.1802918910980225,2.172802209854126,2.2370128631591797,2.2163522243499756,2.1917192935943604,2.188493251800537,2.172468423843384,2.1912224292755127,2.15193510055542,2.1854796409606934,2.1579489707946777,2.1521923542022705,2.2163679599761963,2.1739559173583984,2.20697283744812,2.123627185821533,2.1680405139923096,2.1364786624908447,2.1631581783294678,2.046008348464966,2.155013084411621,2.1623952388763428,2.1894795894622803,2.11194109916687,2.0747594833374023,2.174830198287964,2.0998754501342773,2.119666576385498,2.1083319187164307,2.161661386489868,2.0107786655426025,2.0861995220184326,2.1000149250030518,2.097852945327759,2.09883713722229,2.093020439147949,2.1174628734588623,2.0633699893951416,2.0367484092712402,2.1043081283569336,2.059217691421509,2.061941146850586,2.0855278968811035,2.025348424911499,2.037827968597412,2.022026300430298,2.1252989768981934,1.9917116165161133,2.0235865116119385,1.9728968143463135,1.972288727760315,1.9422599077224731,1.871329426765442,2.04823637008667,2.01029372215271,1.9609838724136353,2.0013692378997803,2.0597429275512695,2.065117120742798,2.0310449600219727,1.9596896171569824,1.9395806789398193,2.005828857421875,1.9137020111083984,1.9111655950546265,1.9217047691345215,1.9816694259643555,2.041524648666382,2.0261292457580566,1.9598183631896973,2.086426258087158,1.8857077360153198,1.9477046728134155,1.8772622346878052,1.9189846515655518,1.907932162284851,1.8594599962234497,1.900428056716919,1.9342551231384277,1.84811270236969,1.937400460243225,1.9643536806106567,1.8474469184875488,1.7728148698806763,1.8374450206756592,1.7854243516921997,2.017177104949951,1.8069545030593872,1.7810742855072021,1.9662954807281494,1.8244186639785767,1.9450962543487549,1.6831327676773071,1.7641000747680664,1.8526506423950195,1.751612663269043,1.767112374305725,1.712140440940857,1.743030309677124,1.7544536590576172,1.7630735635757446,1.7955700159072876,1.7248575687408447,1.7860435247421265,1.7747739553451538,1.7874486446380615,1.7295095920562744,1.8073303699493408,1.7456316947937012,1.6884195804595947,1.6995817422866821,1.6046432256698608,1.659032940864563,1.7461175918579102,1.4998705387115479,1.7364459037780762,1.790802001953125,1.6420819759368896,1.7007758617401123,1.6137449741363525,1.6159850358963013,1.5320236682891846,1.840738296508789,1.664948582649231,1.533339500427246,1.550398826599121,1.2992080450057983,1.6100430488586426,1.7971172332763672,1.5627927780151367,1.424424171447754,1.3806591033935547,1.510953664779663,1.3238558769226074,1.6253151893615723,1.5683704614639282,1.555135726928711,1.5079265832901,1.5465019941329956,1.3827556371688843,1.5976680517196655,1.5143368244171143,1.4270315170288086,1.3427118062973022,1.4203284978866577,1.4320372343063354,1.4741657972335815,1.4564132690429688,1.4244170188903809,1.2997612953186035,1.2170159816741943,1.324830174446106,1.5553388595581055,1.34443998336792,1.5546207427978516,1.4155457019805908,1.3413349390029907,1.3357094526290894,1.3118027448654175,1.4170910120010376,1.3888641595840454,1.3121602535247803,1.3922227621078491,1.279526710510254,1.3227688074111938,1.6000218391418457,1.317060112953186,1.3785409927368164,1.333946943283081,1.5266835689544678,1.3341377973556519,1.3036731481552124,1.4873785972595215,1.2606698274612427,1.3189809322357178,1.3412333726882935,1.3398419618606567,1.557803988456726,1.4805073738098145,1.2985260486602783],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"LightSeaGreen\",\"size\":20},\"mode\":\"markers\",\"x\":[0.0,60000.0,120000.0,180000.0],\"y\":[2.3218202392578124,2.1867050048828127,1.6571057373046876,0.9317846923828125],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"x\":[0.0,640.0,1280.0,1920.0,2560.0,3200.0,3840.0,4480.0,5120.0,5760.0,6400.0,7040.0,7680.0,8320.0,8960.0,9600.0,10240.0,10880.0,11520.0,12160.0,12800.0,13440.0,14080.0,14720.0,15360.0,16000.0,16640.0,17280.0,17920.0,18560.0,19200.0,19840.0,20480.0,21120.0,21760.0,22400.0,23040.0,23680.0,24320.0,24960.0,25600.0,26240.0,26880.0,27520.0,28160.0,28800.0,29440.0,30080.0,30720.0,31360.0,32000.0,32640.0,33280.0,33920.0,34560.0,35200.0,35840.0,36480.0,37120.0,37760.0,38400.0,39040.0,39680.0,40320.0,40960.0,41600.0,42240.0,42880.0,43520.0,44160.0,44800.0,45440.0,46080.0,46720.0,47360.0,48000.0,48640.0,49280.0,49920.0,50560.0,51200.0,51840.0,52480.0,53120.0,53760.0,54400.0,55040.0,55680.0,56320.0,56960.0,57600.0,58240.0,58880.0,59520.0,60000.0,60640.0,61280.0,61920.0,62560.0,63200.0,63840.0,64480.0,65120.0,65760.0,66400.0,67040.0,67680.0,68320.0,68960.0,69600.0,70240.0,70880.0,71520.0,72160.0,72800.0,73440.0,74080.0,74720.0,75360.0,76000.0,76640.0,77280.0,77920.0,78560.0,79200.0,79840.0,80480.0,81120.0,81760.0,82400.0,83040.0,83680.0,84320.0,84960.0,85600.0,86240.0,86880.0,87520.0,88160.0,88800.0,89440.0,90080.0,90720.0,91360.0,92000.0,92640.0,93280.0,93920.0,94560.0,95200.0,95840.0,96480.0,97120.0,97760.0,98400.0,99040.0,99680.0,100320.0,100960.0,101600.0,102240.0,102880.0,103520.0,104160.0,104800.0,105440.0,106080.0,106720.0,107360.0,108000.0,108640.0,109280.0,109920.0,110560.0,111200.0,111840.0,112480.0,113120.0,113760.0,114400.0,115040.0,115680.0,116320.0,116960.0,117600.0,118240.0,118880.0,119520.0,120000.0,120640.0,121280.0,121920.0,122560.0,123200.0,123840.0,124480.0,125120.0,125760.0,126400.0,127040.0,127680.0,128320.0,128960.0,129600.0,130240.0,130880.0,131520.0,132160.0,132800.0,133440.0,134080.0,134720.0,135360.0,136000.0,136640.0,137280.0,137920.0,138560.0,139200.0,139840.0,140480.0,141120.0,141760.0,142400.0,143040.0,143680.0,144320.0,144960.0,145600.0,146240.0,146880.0,147520.0,148160.0,148800.0,149440.0,150080.0,150720.0,151360.0,152000.0,152640.0,153280.0,153920.0,154560.0,155200.0,155840.0,156480.0,157120.0,157760.0,158400.0,159040.0,159680.0,160320.0,160960.0,161600.0,162240.0,162880.0,163520.0,164160.0,164800.0,165440.0,166080.0,166720.0,167360.0,168000.0,168640.0,169280.0,169920.0,170560.0,171200.0,171840.0,172480.0,173120.0,173760.0,174400.0,175040.0,175680.0,176320.0,176960.0,177600.0,178240.0,178880.0,179520.0],\"y\":[2.3214004039764404,2.2491350173950195,2.113179922103882,1.9550843238830566,1.6728605031967163,1.217984914779663,1.282928228378296,1.0469459295272827,1.0517696142196655,1.0507243871688843,0.6877237558364868,0.9348399639129639,0.591903030872345,0.7308865785598755,0.6827392578125,0.6268460750579834,0.4449503421783447,0.6774118542671204,0.6748184561729431,0.8150025010108948,0.3500133752822876,0.65462327003479,0.42191416025161743,0.4529464542865753,0.4703286588191986,0.40873122215270996,0.4525415003299713,0.5031951665878296,0.287374347448349,0.42254260182380676,0.2717137336730957,0.5210371017456055,1.029237985610962,0.4640721380710602,0.4952549636363983,0.6679432988166809,0.44027093052864075,0.3999949097633362,0.32858702540397644,0.2920404374599457,0.37941521406173706,0.4174332022666931,0.4874421954154968,0.5118772387504578,0.3725377321243286,0.44351688027381897,0.38601815700531006,0.325380802154541,0.36968204379081726,0.41653504967689514,0.21858440339565277,0.4117812514305115,0.43975403904914856,0.4841625988483429,0.44047459959983826,0.47812578082084656,0.6502758264541626,0.3301624357700348,0.3042009174823761,0.1790051907300949,0.4764031767845154,0.343373566865921,0.27964016795158386,0.41281628608703613,0.3617386817932129,0.26855596899986267,0.20782195031642914,0.41112518310546875,0.18771801888942719,0.45900240540504456,0.30674612522125244,0.23273561894893646,0.2910745441913605,0.2661411166191101,0.2116415947675705,0.22955292463302612,0.4345496892929077,0.15759551525115967,0.4292997121810913,0.3865722119808197,0.3015410602092743,0.6660727262496948,0.3756412863731384,0.31666696071624756,0.29663604497909546,0.37881171703338623,0.22218210995197296,0.2731504440307617,0.4625087380409241,0.25209665298461914,0.2849139869213104,0.2481159269809723,0.20142284035682678,0.4829024374485016,0.39614424109458923,0.1767241358757019,0.2529141306877136,0.3615280091762543,0.13081760704517365,0.14613957703113556,0.24329280853271484,0.29210278391838074,0.3044636845588684,0.3228464424610138,0.3045092821121216,0.21384592354297638,0.22708450257778168,0.3067120313644409,0.10650212317705154,0.32049012184143066,0.2618655264377594,0.19014663994312286,0.19528983533382416,0.41878145933151245,0.5026137828826904,0.2516389489173889,0.27004364132881165,0.2430080771446228,0.346963107585907,0.2884218394756317,0.35233044624328613,0.2981458008289337,0.18084432184696198,0.2852504551410675,0.2026532143354416,0.312748521566391,0.18083517253398895,0.3021516799926758,0.24932338297367096,0.15447695553302765,0.27461451292037964,0.1736864596605301,0.2266368567943573,0.25028494000434875,0.15949499607086182,0.28982099890708923,0.1341623216867447,0.2299758642911911,0.20505498349666595,0.1667732447385788,0.15454474091529846,0.31794437766075134,0.16763870418071747,0.4143206477165222,0.2568933367729187,0.14662453532218933,0.1602577269077301,0.2847284972667694,0.11815118789672852,0.2948513925075531,0.4639278054237366,0.4147482216358185,0.09169597178697586,0.3266255557537079,0.11011849343776703,0.4671580493450165,0.20057031512260437,0.10138461738824844,0.3715053200721741,0.09291742742061615,0.35138222575187683,0.1754789501428604,0.13569232821464539,0.19787348806858063,0.20285342633724213,0.19980044662952423,0.2708267569541931,0.26609474420547485,0.1352539211511612,0.3333044946193695,0.3518303632736206,0.2449197918176651,0.2711598873138428,0.15198339521884918,0.13963022828102112,0.15035240352153778,0.22614730894565582,0.16112317144870758,0.23547273874282837,0.35986053943634033,0.26162979006767273,0.24376466870307922,0.26816126704216003,0.19865988194942474,0.27475807070732117,0.18211741745471954,0.25164180994033813,0.16452334821224213,0.14985716342926025,0.10076702386140823,0.1961304396390915,0.1786416471004486,0.05268128216266632,0.175081267952919,0.2091982215642929,0.25414955615997314,0.27105793356895447,0.29540225863456726,0.2560819387435913,0.2516554296016693,0.20178534090518951,0.09645885974168777,0.2228766232728958,0.10159037262201309,0.26571932435035706,0.1838735193014145,0.32829171419143677,0.3404238820075989,0.17612236738204956,0.17425869405269623,0.08696821331977844,0.13897587358951569,0.1286408007144928,0.0958814024925232,0.24556975066661835,0.6521446108818054,0.10050838440656662,0.13030654191970825,0.08924645930528641,0.2760757803916931,0.1321721225976944,0.24543149769306183,0.13620415329933167,0.13585618138313293,0.2972187101840973,0.11181566119194031,0.18248198926448822,0.18496902287006378,0.08992715924978256,0.1315208375453949,0.314932256937027,0.24503612518310547,0.16142140328884125,0.13655194640159607,0.3147802948951721,0.2850138545036316,0.2427360862493515,0.29966792464256287,0.23498855531215668,0.10150351375341415,0.10156218707561493,0.1723816692829132,0.08646003156900406,0.08355537056922913,0.4201597571372986,0.2572023570537567,0.23882709443569183,0.13382074236869812,0.11682209372520447,0.2776638865470886,0.34000080823898315,0.08338617533445358,0.05135013535618782,0.29232490062713623,0.5210257172584534,0.16468437016010284,0.09702695161104202,0.25252166390419006,0.17083118855953217,0.2888200283050537,0.3448212146759033,0.2634279131889343,0.19172167778015137,0.12461455166339874,0.11982240527868271,0.11278615891933441,0.3844338059425354,0.2194102704524994,0.33914607763290405,0.20327036082744598,0.39716559648513794,0.32462215423583984,0.3671645224094391,0.22501833736896515,0.17778244614601135,0.1281421035528183,0.17498528957366943,0.2004949450492859,0.35072728991508484,0.250206857919693,0.17244961857795715,0.11453812569379807],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"LightSeaGreen\",\"size\":20},\"mode\":\"markers\",\"x\":[0.0,60000.0,120000.0,180000.0],\"y\":[2.2976204345703124,0.09745763702392578,0.06881285743713379,0.061079143524169925],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"x\":[0.0,640.0,1280.0,1920.0,2560.0,3200.0,3840.0,4480.0,5120.0,5760.0,6400.0,7040.0,7680.0,8320.0,8960.0,9600.0,10240.0,10880.0,11520.0,12160.0,12800.0,13440.0,14080.0,14720.0,15360.0,16000.0,16640.0,17280.0,17920.0,18560.0,19200.0,19840.0,20480.0,21120.0,21760.0,22400.0,23040.0,23680.0,24320.0,24960.0,25600.0,26240.0,26880.0,27520.0,28160.0,28800.0,29440.0,30080.0,30720.0,31360.0,32000.0,32640.0,33280.0,33920.0,34560.0,35200.0,35840.0,36480.0,37120.0,37760.0,38400.0,39040.0,39680.0,40320.0,40960.0,41600.0,42240.0,42880.0,43520.0,44160.0,44800.0,45440.0,46080.0,46720.0,47360.0,48000.0,48640.0,49280.0,49920.0,50560.0,51200.0,51840.0,52480.0,53120.0,53760.0,54400.0,55040.0,55680.0,56320.0,56960.0,57600.0,58240.0,58880.0,59520.0,60000.0,60640.0,61280.0,61920.0,62560.0,63200.0,63840.0,64480.0,65120.0,65760.0,66400.0,67040.0,67680.0,68320.0,68960.0,69600.0,70240.0,70880.0,71520.0,72160.0,72800.0,73440.0,74080.0,74720.0,75360.0,76000.0,76640.0,77280.0,77920.0,78560.0,79200.0,79840.0,80480.0,81120.0,81760.0,82400.0,83040.0,83680.0,84320.0,84960.0,85600.0,86240.0,86880.0,87520.0,88160.0,88800.0,89440.0,90080.0,90720.0,91360.0,92000.0,92640.0,93280.0,93920.0,94560.0,95200.0,95840.0,96480.0,97120.0,97760.0,98400.0,99040.0,99680.0,100320.0,100960.0,101600.0,102240.0,102880.0,103520.0,104160.0,104800.0,105440.0,106080.0,106720.0,107360.0,108000.0,108640.0,109280.0,109920.0,110560.0,111200.0,111840.0,112480.0,113120.0,113760.0,114400.0,115040.0,115680.0,116320.0,116960.0,117600.0,118240.0,118880.0,119520.0,120000.0,120640.0,121280.0,121920.0,122560.0,123200.0,123840.0,124480.0,125120.0,125760.0,126400.0,127040.0,127680.0,128320.0,128960.0,129600.0,130240.0,130880.0,131520.0,132160.0,132800.0,133440.0,134080.0,134720.0,135360.0,136000.0,136640.0,137280.0,137920.0,138560.0,139200.0,139840.0,140480.0,141120.0,141760.0,142400.0,143040.0,143680.0,144320.0,144960.0,145600.0,146240.0,146880.0,147520.0,148160.0,148800.0,149440.0,150080.0,150720.0,151360.0,152000.0,152640.0,153280.0,153920.0,154560.0,155200.0,155840.0,156480.0,157120.0,157760.0,158400.0,159040.0,159680.0,160320.0,160960.0,161600.0,162240.0,162880.0,163520.0,164160.0,164800.0,165440.0,166080.0,166720.0,167360.0,168000.0,168640.0,169280.0,169920.0,170560.0,171200.0,171840.0,172480.0,173120.0,173760.0,174400.0,175040.0,175680.0,176320.0,176960.0,177600.0,178240.0,178880.0,179520.0],\"y\":[2.2875115871429443,2.300504446029663,2.2359979152679443,2.242865562438965,2.190251111984253,2.197073221206665,2.1167891025543213,2.1088132858276367,2.10612154006958,2.012726306915283,2.0518274307250977,1.9625295400619507,1.907873511314392,1.9508837461471558,1.9140722751617432,1.8175441026687622,1.7145938873291016,1.8309022188186646,1.6548675298690796,1.8176357746124268,1.7118815183639526,1.6894080638885498,1.589697003364563,1.6797775030136108,1.6462002992630005,1.5517778396606445,1.6133215427398682,1.5234365463256836,1.3927782773971558,1.6747376918792725,1.4573901891708374,1.606515884399414,1.6398468017578125,1.4814035892486572,1.3150686025619507,1.5589094161987305,1.4978728294372559,1.294116497039795,1.378974437713623,1.5040278434753418,1.272802710533142,1.49551522731781,1.2574491500854492,1.2614312171936035,1.054988980293274,1.450048804283142,1.061309814453125,1.2427377700805664,1.265560269355774,1.2911162376403809,1.2133228778839111,1.267582893371582,1.42804753780365,1.1677249670028687,1.2558428049087524,1.1844441890716553,1.2556426525115967,1.0963436365127563,1.257606863975525,1.1511725187301636,1.2861071825027466,0.9520843029022217,1.1504061222076416,1.1528409719467163,1.0368770360946655,0.8551959991455078,1.075757622718811,1.1462492942810059,1.220244288444519,1.297755479812622,1.1396560668945312,1.1898248195648193,1.0742899179458618,1.0418307781219482,1.0909240245819092,1.0506510734558105,0.9414018392562866,1.0829570293426514,1.1716525554656982,1.2084920406341553,1.2641807794570923,1.1336287260055542,0.8971878290176392,1.0555617809295654,0.9612958431243896,0.981146514415741,0.9773836731910706,1.0273034572601318,0.9430421590805054,1.1695599555969238,0.8337281346321106,0.7682875394821167,0.835303544998169,1.2128422260284424,0.9120358824729919,0.9450220465660095,0.892268180847168,0.9258137941360474,0.9251459836959839,0.8385780453681946,0.8454325199127197,1.0011996030807495,1.077707052230835,1.0706874132156372,0.9561939239501953,0.8909566402435303,0.7002273797988892,0.9008445739746094,0.7653219699859619,0.869125485420227,0.9284773468971252,0.7797060012817383,0.7336468696594238,1.1288726329803467,0.862496554851532,0.7846869826316833,0.7858154773712158,0.9015138745307922,0.9861415028572083,0.9843314290046692,0.8900245428085327,0.8864097595214844,0.858707070350647,0.857146143913269,0.8865261077880859,0.8781697154045105,1.0280603170394897,0.8653281331062317,0.8639011383056641,0.9876275658607483,0.8521796464920044,0.9192419052124023,1.0440248250961304,1.0503441095352173,0.7012239098548889,1.0301352739334106,0.8564425706863403,0.7633054256439209,0.8944963812828064,0.8001094460487366,0.7142788171768188,0.9894620180130005,0.8153818845748901,0.8835431933403015,1.1272778511047363,0.8098727464675903,0.790043830871582,0.948943018913269,1.084686517715454,0.5981951951980591,0.9707440137863159,0.9333227276802063,0.7717932462692261,0.7922369837760925,0.7271298766136169,0.979376494884491,0.6977187991142273,0.7260568737983704,0.9004344344139099,0.8136488199234009,1.033410906791687,0.7890942096710205,0.6222870349884033,0.9593895673751831,0.7823308110237122,0.7931990027427673,0.8479955792427063,0.8239649534225464,0.7626593708992004,0.7792294025421143,0.6921899914741516,0.6280497908592224,0.8041568398475647,0.9005836844444275,0.7679165005683899,0.7274289727210999,0.762522280216217,0.729101300239563,0.8559996485710144,0.8124387264251709,0.7705483436584473,0.8105572462081909,0.8678378462791443,0.910071611404419,1.0075325965881348,0.8019163608551025,1.116752028465271,0.8081250786781311,0.6793834567070007,0.9268950819969177,0.6652907729148865,1.0007874965667725,0.6790573596954346,0.7746814489364624,0.5141255259513855,0.7361865043640137,0.620993971824646,0.8206615447998047,0.8000737428665161,0.9079744815826416,0.6412835717201233,0.8283942341804504,0.8640958666801453,0.6981977820396423,0.7930024862289429,0.7558342218399048,0.9245433807373047,0.8593810200691223,0.7638619542121887,0.8297258019447327,0.9704251885414124,0.693073570728302,0.5496518015861511,0.5882156491279602,0.7925745844841003,0.8012880086898804,0.5718095302581787,0.8414411544799805,0.9035210013389587,0.9280718564987183,0.9105503559112549,0.8058755993843079,0.8312736749649048,0.5994673371315002,0.8527539968490601,0.7157631516456604,0.7418820858001709,0.5645716786384583,0.7046452760696411,0.9662256836891174,0.7733017802238464,0.8261150121688843,0.6222828030586243,0.7270185947418213,0.8163397908210754,0.6858680248260498,0.8468042016029358,0.8609387278556824,0.7472202181816101,0.6133138537406921,0.5840766429901123,0.5763075351715088,0.8250061273574829,0.5826945304870605,0.7596682906150818,0.7104973196983337,0.5650103688240051,0.6196257472038269,0.854304313659668,0.9211376309394836,0.6827626824378967,0.6789737343788147,0.5109963417053223,0.7003975510597229,0.9455483555793762,0.6797776222229004,0.5940417051315308,0.7408004999160767,0.7230597734451294,0.7484133243560791,0.7235956788063049,0.7203028202056885,0.7148546576499939,0.722791314125061,0.8017765283584595,0.6220518946647644,0.7607918977737427,0.8309109210968018,0.8639130592346191,0.8372400403022766,0.6239771842956543,0.9306585192680359,0.8426128625869751,0.6961178779602051,0.9304438829421997,0.6842623949050903,0.8008204102516174,0.5936208963394165,0.6707947850227356,1.114287257194519,0.6608588695526123,0.6466997861862183],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"LightSeaGreen\",\"size\":20},\"mode\":\"markers\",\"x\":[0.0,60000.0,120000.0,180000.0],\"y\":[2.30438857421875,0.6094117004394531,0.4482239562988281,0.37926560668945314],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.88,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.66,0.78]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.44,0.56]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.0,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.22,0.33999999999999997]},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,1.0]},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.12]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Plot SGD\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Plot Adam\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.78,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Plot Adagrad\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.56,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Plot RMSprop\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.33999999999999997,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Plot Adadelta\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.12,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Optimizers\"},\"height\":500,\"width\":700},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f0974c26-2089-479e-b1bd-6a43791aa5f1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}